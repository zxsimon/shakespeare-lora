from itertools import islice
import datasets
import torch
from tqdm import tqdm


def generator_mmlu(split="test", max_examples=1000):
    """Generator for MMLU dataset. Chat template is applied to the prompt to encourage immediate categorical output."""

    mmlu = datasets.load_dataset("cais/mmlu", "all")
    mmlu = mmlu[split].shuffle().select(range(max_examples))

    for doc in mmlu:
        question = doc["question"]
        choices = doc["choices"]
        prompt = f"<|im_start|>user\nAnswer the following question:\n{question}\n"
        prompt += "".join(f"-{choice}={character}\n" for choice, character in zip(choices, "ABCD"))
        prompt += "Output ONLY and IMMEDIATELY the SINGLE letter corresponding to the correct answer. No other text or prefix.<|im_end|>\n"
        prompt += "<|im_start|>assistant<think>\n\n</think>\n\n"

        yield prompt, doc["answer"]

@torch.no_grad()
def evaluate_mmlu(model, tokenizer, batch_size = 4, total_examples = 8, show_modal_tokens = False, generator = None):
    """Evaluate the model on the MMLU dataset."""

    if generator is None:
        generator = generator_mmlu(max_examples=total_examples)

    answer_set = ["A", "B", "C", "D"]
    answer_token_ids = tokenizer(answer_set, return_tensors="pt")["input_ids"].squeeze() # 4

    total_evaluated = 0
    total_correct = 0
    num_iters = total_examples // batch_size

    for _ in tqdm(range(num_iters), total=num_iters, desc="Evaluating MMLU"):

        prompts, answers = map(list, zip(*islice(generator, batch_size)))
        answers = torch.tensor(answers).to(model.device)
        tokenized_prompts = tokenizer(prompts, return_tensors="pt", padding=True, pad_to_multiple_of=8).to(model.device)
        input_ids, attention_mask = tokenized_prompts["input_ids"], tokenized_prompts["attention_mask"]

        with torch.autocast(device_type=model.device.type, dtype=torch.bfloat16):
            logits = model(input_ids).logits # B, T, C

        # Get the last logits of the input sequence for each example
        # Note that next logit indices are different due to varying sequence lengths
        last_idx = attention_mask.sum(dim=1) - 1 # B
        batch_indices = torch.arange(logits.shape[0], device=logits.device)
        next_logit = logits[batch_indices, last_idx, :]
        
        # For debugging purposes to see the modal next tokens generated by the model
        if show_modal_tokens:
            modal_tokens = tokenizer.batch_decode(next_logit.argmax(dim=-1).squeeze())
            print(modal_tokens)
        
        choices = next_logit[..., answer_token_ids].argmax(dim=-1).squeeze() # B
        num_correct = (choices == answers).sum().item()
        total_correct += num_correct
        total_evaluated += batch_size

        if model.device.type == "mps":
            torch.mps.empty_cache()

    return total_correct / total_evaluated

if __name__ == "__main__":
    # Testing
    from transformers import AutoTokenizer, AutoModelForCausalLM
    model_name = "Qwen/Qwen3-0.6B"
    model = AutoModelForCausalLM.from_pretrained(model_name, dtype=torch.bfloat16)
    device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
    model = model.to(device)
    if torch.cuda.is_available():
        model = torch.compile(model)
        torch.set_float32_matmul_precision('high')
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    print(evaluate_mmlu(model, tokenizer, batch_size=4, total_examples=100, show_modal_tokens=True))